{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa65d751",
   "metadata": {},
   "source": [
    "# Session 3 — Sentence-Level Analysis\n",
    "## Measure 3: Sentence Embeddings (LLMs as Semantic Encoders)\n",
    "\n",
    "### What are Sentence Embeddings?\n",
    "Sentence embeddings are **dense vector representations** of sentences that capture their semantic meaning in a high-dimensional space. Unlike traditional word-based approaches, sentence embeddings encode the entire meaning of a sentence into a single fixed-length vector (typically 384-768 dimensions).\n",
    "\n",
    "### How They Work\n",
    "Modern sentence embedding models (like SBERT - Sentence-BERT) use **transformer architectures** pre-trained on massive text corpora. These models:\n",
    "- Convert sentences into numerical vectors where semantically similar sentences are close together in vector space\n",
    "- Capture context, syntax, and semantic relationships\n",
    "- Enable comparison of sentences based on meaning rather than just word overlap\n",
    "\n",
    "### Key Applications in NLP\n",
    "1. **Semantic Similarity**: Measure how similar two sentences are in meaning\n",
    "2. **Information Retrieval**: Find relevant documents or passages based on semantic search\n",
    "3. **Text Clustering**: Group similar sentences or documents together\n",
    "4. **Duplicate Detection**: Identify paraphrases or semantically identical content\n",
    "5. **Question Answering**: Match questions with relevant answers\n",
    "6. **Content Recommendation**: Suggest similar content based on semantic understanding\n",
    "\n",
    "### Why Use Sentence Embeddings?\n",
    "- **Beyond Keywords**: Captures meaning even when different words are used\n",
    "- **Efficient**: Pre-computed embeddings allow fast similarity comparisons\n",
    "- **Robust**: Works across different phrasings and writing styles\n",
    "- **Multilingual**: Many models support cross-lingual semantic search\n",
    "\n",
    "### This Demonstration\n",
    "We'll use the **all-MiniLM-L6-v2** model to:\n",
    "- Encode example sentences into semantic vectors\n",
    "- Visualize their relationships in 2D space using PCA\n",
    "- Compute similarity scores between sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04e63d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (8, 384)\n",
      "Each sentence is represented as a 384-dimensional vector\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap \n",
    "\n",
    "# Load pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# --- NEW SENTENCES: Curated examples from the Tolkien Universe ---\n",
    "sentences = [\n",
    "    \"Frodo hid the Ring quickly, fearing the dark power.\", \n",
    "    \"The dark power of the Ring made Frodo quickly hide it.\", \n",
    "    \"Gandalf spoke of the great journey that lay before them.\", \n",
    "    \"The long quest stretched out before the weary travelers.\", \n",
    "    \"The Orcs were rushing up the hill, shrieking and cursing.\", \n",
    "    \"They were suddenly quiet; only the wind whistled softly.\", \n",
    "    \"Gimli raised his axe, prepared to face the foe.\", \n",
    "    \"Elrond held the council to decide the fate of Middle-earth.\" \n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"Each sentence is represented as a {embeddings.shape[1]}-dimensional vector\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465bbe63",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (238748915.py, line 11)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mcmap='tab10', s=200, alpha=0.6, edgecolors='black', linewidth=1.5)\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "# Create a more comprehensive visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# --- Plot 1: 2D PCA Visualization ---\n",
    "pca = PCA(n_components=2)\n",
    "points = pca.fit_transform(embeddings)\n",
    "\n",
    "ax1 = axes[0]\n",
    "scatter = ax1.scatter(points[:, 0], points[:, 1], c=range(len(sentences)), \n",
    "                      cmap='tab10', s=200, alpha=0.6, edgecolors='black', linewidth=1.5)\n",
    "\n",
    "# Add labels with better positioning\n",
    "for i, (s, (x, y)) in enumerate(zip(sentences, points)):\n",
    "    # Truncate long sentences for readability\n",
    "    label = s if len(s) < 35 else s[:32] + \"...\"\n",
    "    ax1.annotate(label, (x, y), fontsize=9, ha='center', \n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7, edgecolor='gray'),\n",
    "                xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=11)\n",
    "ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=11)\n",
    "ax1.set_title(\"Sentence Embeddings in 2D Semantic Space\\n(via PCA dimension reduction)\", \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Plot 2: Similarity Heatmap ---\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "similarity_matrix = util.cos_sim(embeddings, embeddings).numpy()\n",
    "\n",
    "ax2 = axes[1]\n",
    "# Create custom colormap\n",
    "colors = ['#d73027', '#fee08b', '#d9ef8b', '#1a9850']\n",
    "n_bins = 100\n",
    "cmap = LinearSegmentedColormap.from_list('similarity', colors, N=n_bins)\n",
    "\n",
    "im = ax2.imshow(similarity_matrix, cmap=cmap, aspect='auto', vmin=0, vmax=1)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax2, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Cosine Similarity', rotation=270, labelpad=20, fontsize=11)\n",
    "\n",
    "# Set ticks and labels\n",
    "ax2.set_xticks(range(len(sentences)))\n",
    "ax2.set_yticks(range(len(sentences)))\n",
    "short_labels = [f\"S{i+1}\" for i in range(len(sentences))]\n",
    "ax2.set_xticklabels(short_labels, fontsize=10)\n",
    "ax2.set_yticklabels(short_labels, fontsize=10)\n",
    "\n",
    "# Add similarity scores as text\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        text_color = 'white' if similarity_matrix[i, j] < 0.5 else 'black'\n",
    "        ax2.text(j, i, f'{similarity_matrix[i, j]:.2f}',\n",
    "                ha=\"center\", va=\"center\", color=text_color, fontsize=9)\n",
    "\n",
    "ax2.set_title(\"Pairwise Semantic Similarity Matrix\\n(Cosine similarity of embeddings)\", \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel(\"Sentence ID\", fontsize=11)\n",
    "ax2.set_ylabel(\"Sentence ID\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the sentences for reference\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SENTENCE REFERENCE:\")\n",
    "print(\"=\"*80)\n",
    "for i, sent in enumerate(sentences):\n",
    "    print(f\"S{i+1}: {sent}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ebf308",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "**Left Plot - 2D Semantic Space:**\n",
    "- Sentences with similar meanings cluster together in the vector space\n",
    "- Distance between points indicates semantic dissimilarity\n",
    "- PCA reduces the 384-dimensional embeddings to 2D for visualization\n",
    "- Note: Some information is lost in dimension reduction (see variance percentages)\n",
    "\n",
    "**Right Plot - Similarity Matrix:**\n",
    "- Values range from 0 (completely dissimilar) to 1 (identical)\n",
    "- Diagonal values are always 1.0 (sentence compared to itself)\n",
    "- Higher values (green) indicate semantically similar sentences\n",
    "- Lower values (red) indicate semantically different sentences\n",
    "\n",
    "**Key Observations:**\n",
    "- Paraphrases (e.g., \"Alice looked at the cat\" vs \"The cat stared back at Alice\") show high similarity\n",
    "- Semantically related sentences cluster together in both visualizations\n",
    "- Unrelated sentences have low similarity scores despite being grammatically correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a7dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentence transformer model...\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d46bef246334b7191330a7f994b1e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding shape: (50, 384)\n",
      "Each sentence → 384-dimensional vector\n"
     ]
    }
   ],
   "source": [
    "# Example: Finding most similar sentence pairs\n",
    "print(\"\\nTOP 5 MOST SIMILAR SENTENCE PAIRS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get upper triangle indices (avoid duplicates and self-comparisons)\n",
    "pairs = []\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(i+1, len(sentences)):\n",
    "        pairs.append((i, j, similarity_matrix[i, j]))\n",
    "\n",
    "# Sort by similarity score\n",
    "pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Display top 5\n",
    "for rank, (i, j, score) in enumerate(pairs[:5], 1):\n",
    "    print(f\"{rank}. Similarity: {score:.4f}\")\n",
    "    print(f\"   S{i+1}: {sentences[i]}\")\n",
    "    print(f\"   S{j+1}: {sentences[j]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a18aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
